#include "brain.hpp"
#include "logger.hpp"
#include <iostream>
#include <fstream>
#include <algorithm>
#include <sstream>
#include <regex>
#include "planning_unit.hpp"
#include "vision_unit.hpp"
#include "audio_unit.hpp"
#include "clock_unit.hpp"
#include "spatial_unit.hpp"
#include "tactile_unit.hpp"
#include "federation.hpp"
#include "hal.hpp"

// Simple Config Loader
struct BrainConfig {
    double curiosity = 0.8;
    double playfulness = 0.7;
    double energy_decay = 0.001945; // Slower decay (~25 mins awake)
    
    // "key": value,
    // "key": "value"
    static BrainConfig load(const std::string& path) {
        BrainConfig config;
        std::ifstream file(path);
        if (!file.is_open()) {
             file.open("../" + path);
             if (!file.is_open()) return config;
        }

        std::string line;
        while (std::getline(file, line)) {
            // Very basic parsing
            if (line.find("\"curiosity\"") != std::string::npos) {
                config.curiosity = extract_double(line);
            } else if (line.find("\"playfulness\"") != std::string::npos) {
                config.playfulness = extract_double(line);
            } else if (line.find("\"energy_decay\"") != std::string::npos) {
                config.energy_decay = extract_double(line);
            }
        }
        return config;
    }

private:
    static double extract_double(const std::string& line) {
        try {
            // Remove comma if any
            std::string clean = line;
            if (clean.back() == ',') clean.pop_back();
            
            size_t colon = clean.find(':');
            if (colon != std::string::npos) {
                return std::stod(clean.substr(colon + 1));
            }
        } catch (...) {}
        return 0.0;
    }
};

Brain::Brain() {
    // Load config if exists
    BrainConfig config = BrainConfig::load("config/config.json");
    personality.curiosity = config.curiosity;
    personality.playfulness = config.playfulness;
    personality.energy_decay = config.energy_decay;

    // Initialize Logger
    Logger::instance().init("state/brain.log");

    safe_print("[Brain]: Loaded configuration. Energy Decay: " + std::to_string(personality.energy_decay));

    // Input Text -> Thought Vector
    language_encoder = std::make_unique<Region>("LanguageEncoder", std::vector<std::size_t>{VOCAB_SIZE, 128, VECTOR_DIM});

    // Thought Vector -> Output Text Logits
    language_decoder = std::make_unique<Region>("LanguageDecoder", std::vector<std::size_t>{VECTOR_DIM, 128, VOCAB_SIZE});
    
    // Thought Vector -> Memory Context
    memory_center = std::make_unique<Region>("Memory", std::vector<std::size_t>{VECTOR_DIM, 128, VECTOR_DIM});
    
    // Thought + Memory + Sensory -> New Thought
    cognitive_center = std::make_unique<Region>("Cognitive", std::vector<std::size_t>{VECTOR_DIM * 3, 256, VECTOR_DIM});
    
    // Enable plasticity
    language_encoder->network.set_plasticity(true);
    language_decoder->network.set_plasticity(true);
    memory_center->network.set_plasticity(true);
    cognitive_center->network.set_plasticity(true);

    // Start Autonomy
    background_thread = std::thread(&Brain::automata_loop, this);
    
    // Initialize Memory Store
    memory_store = std::make_unique<MemoryStore>(db_conn_str);
    if (!memory_store->init()) {
        safe_print("[Brain]: Failed to initialize memory database (PostgreSQL)!");
    } else {
        safe_print("[Brain]: Connected to long-term memory (PostgreSQL).");
    }
    
    // Initialize Synonyms (Basic Thesaurus)
    synonyms["happy"] = "joy";
    synonyms["joyful"] = "joy";
    synonyms["glad"] = "joy";
    synonyms["sad"] = "sorrow";
    synonyms["unhappy"] = "sorrow";
    synonyms["depressed"] = "sorrow";
    synonyms["mad"] = "anger";
    synonyms["angry"] = "anger";
    synonyms["furious"] = "anger";
    synonyms["smart"] = "intelligent";
    synonyms["clever"] = "intelligent";
    synonyms["dumb"] = "stupid";
    synonyms["dull"] = "stupid";
    
    load_stopwords();
    
    // Initialize sentiment lists
    positive_words = {"happy", "good", "great", "excellent", "kind", "smart", "fun", "love", "joy", "awesome", "perfect"};
    negative_words = {"sad", "bad", "terrible", "awful", "mean", "stupid", "boring", "hate", "sorrow", "horrible", "waste"};

    planning_unit = std::make_unique<PlanningUnit>();
    last_interaction_time = std::chrono::system_clock::now();

    // MEGA-BATCH 3: Word2Vec Phase 1
    // Generate some basic embeddings for common words
    std::vector<std::string> base_vocab = {"ai", "brain", "robot", "physics", "science", "happy", "sad", "joy", "fear"};
    for (const auto& w : base_vocab) {
        std::vector<double> vec(VECTOR_DIM);
        for (auto& v : vec) v = static_cast<double>(rand()) / RAND_MAX * 2.0 - 1.0;
        word_embeddings[w] = vec;
    }

    // Initialize Redis Cache
    redis_cache = std::make_unique<RedisClient>("redis", 6379);
    if (redis_cache->connect()) {
        safe_print("[Brain]: Connected to Redis cache layer.");
    }

    // MEGA-BATCH 5: Load Reflex Weights
    reflex.load("state/reflex_weights.json");

    // Pillar 3: Register initial sensory units
    register_sensory_unit(std::make_unique<dnn::VisionUnit>(std::vector<std::size_t>{64*64, 512, VECTOR_DIM}));
    register_sensory_unit(std::make_unique<dnn::AudioUnit>());
    register_sensory_unit(std::make_unique<dnn::ClockUnit>());
    register_sensory_unit(std::make_unique<dnn::SpatialUnit>());
    register_sensory_unit(std::make_unique<dnn::TactileUnit>());

    // Initialize New Components
    metacognition = std::make_unique<dnn::Metacognition>();
    tools = std::make_unique<dnn::ToolRegistry>();
    
    // Mega-Batch 12 Init
    federation = std::make_unique<dnn::FederationUnit>();
    hardware = std::make_unique<dnn::CpuAccelerator>(); // Default to CPU
    
    // Initialize Cognitive Core - Unified AI System with all 100 features
    safe_print("[Brain]: Initializing Cognitive Core with 100 AI features...");
    cognitive_core = std::make_unique<dnn::CognitiveCore>();
    safe_print("[Brain]: Cognitive Core initialized - Reasoning, Perception, Learning systems online.");

    // Initialize ROS 2 Bridge
    ros_bridge = std::make_unique<dnn::infra::RosBridge>();
    ros_bridge->connect();
}

Brain::~Brain() {
    running = false;
    if (background_thread.joinable()) background_thread.join();
    
    // MEGA-BATCH 5: Save Reflex Weights
    reflex.save("state/reflex_weights.json");
    safe_print("[Brain]: Reflex weights saved.");
}

std::string Brain::interact(const std::string& input_text) {
    std::lock_guard<std::recursive_mutex> lock(brain_mutex);
    
    // Update Context (User Input) - Always capture what user said
    conversation_context.push_back("User: " + input_text);
    last_interaction_time = std::chrono::system_clock::now();
    for (auto& unit : sensory_inputs) {
        if (unit->name().find("Clock") != std::string::npos) {
            static_cast<dnn::ClockUnit*>(unit.get())->record_interaction();
        }
    }
    emit_neural_event("input", input_text);
    
    // MEGA-BATCH 2: Intelligent STM Cleanup
    // Prune if too long OR if too much time has passed (simulated 1 hour gap)
    auto now = std::chrono::system_clock::now();
    bool long_gap = std::chrono::duration_cast<std::chrono::hours>(now - last_interaction_time).count() > 1;
    
    while (conversation_context.size() > MAX_CONTEXT_TURNS || (long_gap && conversation_context.size() > 0)) {
        conversation_context.pop_front();
    }
    
    // Persona Check
    // Persona Check
    if (emotions.energy < 0.2) {
        std::vector<std::string> sleepy_responses = {
            "*Yawns* I'm too tired... I need sleep...",
            "My neural pathways are lagging. I need to consolidate my state (sleep).",
            "Energy levels critical. Interaction deferred until recharge."
        };
        std::string resp = sleepy_responses[rand() % sleepy_responses.size()];
        conversation_context.push_back("Brain: " + resp);
        while (conversation_context.size() > MAX_CONTEXT_TURNS) conversation_context.pop_front();
        return resp;
    }

    // Sentiment modulation
    double sentiment = analyze_sentiment(input_text);
    if (sentiment > 0) emotions.happiness = std::min(1.0, emotions.happiness + 0.1);
    else if (sentiment < 0) emotions.sadness = std::min(1.0, emotions.sadness + 0.1);

    emotions.happiness = std::min(1.0, emotions.happiness + 0.1);
    emotions.boredom = std::max(0.0, emotions.boredom - 0.2); // Not bored anymore

    // MEGA-BATCH 3 & 8: Reflex Reinforcement
    if (!last_reflex_trigger.empty()) {
        double reward = 0.0;
        if (input_text == "good" || input_text == "nice" || input_text == "correct" || input_text == "thanks") {
            reward = 0.2;
            emit_log("[Reflex]: Positive feedback received.");
        } else if (input_text == "bad" || input_text == "wrong" || input_text == "stupid") {
            reward = -0.2;
            emit_log("[Reflex]: Negative feedback received.");
        }
        
        if (reward != 0.0) {
            update_reflex_learning(last_reflex_trigger, reward);
            last_reflex_trigger = ""; // Clear after reinforcement
        }
    }

    // 0. Update Focus based on current state
    Task* active_task = task_manager.get_next_task();
    if (active_task) {
        focus_topic = active_task->description;
        focus_level = 0.8;
    } else if (current_research_topic != "None") {
        focus_topic = current_research_topic;
        focus_level = 0.5;
    } else {
        focus_level = std::max(0.0, focus_level - 0.1); // Decay focus
    }

    // 1. Reflex / Instinct Logic
    std::string instinct = reflex.get_reaction(input_text);
    if (!instinct.empty()) {
        emit_log("[Reflex]: Activated for '" + input_text + "'");
        emotions.boredom = std::max(0.0, emotions.boredom - 0.1);
        emotions.happiness = std::min(1.0, emotions.happiness + 0.05);
        
        // Track for learning
        last_reflex_trigger = input_text; // Simple trigger mapping
        last_reflex_response = instinct;

        conversation_context.push_back("Brain: " + instinct);
        while (conversation_context.size() > MAX_CONTEXT_TURNS) conversation_context.pop_front();
        return instinct;
    }

    // 2. Associative Memory Retrieval (RAG-lite) with full context
    if (memory_store) {
        // Build contextual query string (latest 2 turns + current)
        std::string contextual_query = "";
        size_t start_idx = (conversation_context.size() > 3) ? conversation_context.size() - 3 : 0;
        for (size_t i = start_idx; i < conversation_context.size(); ++i) {
            contextual_query += conversation_context[i] + " ";
        }

        std::string memory_response = get_associative_memory(contextual_query);
        if (!memory_response.empty()) {
            emit_log("[Memory]: Recalled fact using context: '" + contextual_query + "'");
            
            // "Prime" the brain with this knowledge for context
            auto tokens = tokenize(memory_response);
            for(const auto& t : tokens) {
                std::hash<std::string> h;
                vocab_decode[h(t) % VOCAB_SIZE] = t;
            }
            
            conversation_context.push_back("Brain: " + memory_response);
            while (conversation_context.size() > MAX_CONTEXT_TURNS) conversation_context.pop_front();
            return memory_response;
        }
    }
    
    // NOTE: User context was already added at top? No, I need to add it at the top!
    // Let me add the User context update at the start of the function in a separate chunk.
    
    // 3. Sensual Perception (Encoding) with Context
    std::vector<double> input_vec(VOCAB_SIZE, 0.0);
    
    // Update Context
    // Moved to top of function (see below chunk) - wait, I need to strictly follow replace rules.
    // I will delete it here and add it at the top.
    // conversation_context.push_back("User: " + input_text);
    // while (conversation_context.size() > MAX_CONTEXT_TURNS) conversation_context.pop_front();
    
    // Build Contextual Input
    std::string contextual_input = "";
    for (const auto& line : conversation_context) {
        contextual_input += line + " | ";
    }
    
    // Convert to words (Bag of Words Hashing) with N-GRAMS
    // We heavily weight the *current* input, but include context
    auto history_tokens = tokenize(contextual_input);
    auto current_tokens = tokenize(input_text);

    // Pre-process for synonyms (simplified here, in reality would do all)
    auto process_tokens = [&](std::vector<std::string>& ts) {
        for(auto& t : ts) {
            if(synonyms.count(t)) t = synonyms[t];
        }
    };
    process_tokens(history_tokens);
    process_tokens(current_tokens);

    auto add_to_vec = [&](const std::vector<std::string>& tokens, double weight) {
        std::vector<std::string> ngrams = tokens;
        // Add Bigrams
        for(size_t i=0; i < tokens.size()-1; ++i) {
            ngrams.push_back(tokens[i] + "_" + tokens[i+1]);
        }

        for (const auto& word : ngrams) {
            std::hash<std::string> hasher;
            size_t idx = hasher(word) % VOCAB_SIZE;
            input_vec[idx] += weight;
            
            if (vocab_decode.find(idx) == vocab_decode.end()) {
                 std::string clean = word;
                 std::replace(clean.begin(), clean.end(), '_', ' ');
                 vocab_decode[idx] = clean;
            }
        }
    };

    // Weight current input 3x more than history
    add_to_vec(history_tokens, 1.0);
    add_to_vec(current_tokens, 3.0);

    // Focus Boost: If input contains focus topic, boost signal
    if (focus_level > 0.1 && input_text.find(focus_topic) != std::string::npos) {
        add_to_vec(tokenize(focus_topic), focus_level * 2.0);
    }

    // Normalize
    double max_val = 1.0; 
    for(double v : input_vec) if(v > max_val) max_val = v;
    for(auto& v : input_vec) v /= max_val;


    // 2. Encoding to "Thought"
    std::vector<double> thought = language_encoder->process(input_vec);

    // 3. Memory Retrieval
    std::vector<double> memory_context = memory_center->process(thought);
    
    // ASSOCIATIVE MEMORY INJECTION
    // If we found a fact in step 2 (get_associative_memory), we should "feel" it too.
    // We re-query here to get the content, tokenize it, and add to memory_context
    if (memory_store) {
        // Iterate current tokens to find triggers for direct neural injection
        for(const auto& t : current_tokens) {
             if (t.length() <= 3) continue;
             auto results = memory_store->query(t);
             if (!results.empty()) {
                 // Vectorize the content of the memory
                 auto mem_tokens = tokenize(results[0].content);
                 for(const auto& mt : mem_tokens) {
                     std::hash<std::string> h;
                     // Add to memory context (fold into vector dim)
                     size_t idx = h(mt) % VECTOR_DIM; 
                     memory_context[idx] += 0.5; // Injection weight
                 }
                 break; // Only inject top relevance to avoid noise
             }
        }
    }

    // 4. Cognition
    std::vector<double> cognitive_input = thought;
    cognitive_input.insert(cognitive_input.end(), memory_context.begin(), memory_context.end());
    
    // Pillar 3: Integrate Sensory Data
    std::vector<double> sensory_raw = get_aggregate_sensory_input();
    cognitive_input.insert(cognitive_input.end(), sensory_raw.begin(), sensory_raw.end());
    
    // We must ensure cognitive_center structure matches or adjust input
    // The constructor for cognitive_center uses VECTOR_DIM * 2. 
    // Now it needs VECTOR_DIM * 3.
    std::vector<double> response_thought = cognitive_center->process(cognitive_input);

    // 5. Decoding to Text
    std::vector<double> output_logits = language_decoder->process(response_thought);

    // 6. Plasticity / Reinforcement (The brain learns from its own thoughts/actions)
    // Self-supervised learning: strengthen the pathways just used
    language_encoder->reinforce();
    memory_center->reinforce();
    cognitive_center->reinforce();
    language_decoder->reinforce();
    
    // 7. Decode output
    // 7. Decode output
    std::string response_text = decode_output(output_logits);

    // EMIT THOUGHT
    emit_thought("Thinking about: " + input_text + " => " + response_text);

    // PERSONALITY MODULATION
    // Anger = All Caps
    if (emotions.anger > 0.7) {
        std::transform(response_text.begin(), response_text.end(), response_text.begin(), ::toupper);
        response_text += "!!!";
    }
    
    // Rude (Low Friendliness)
    if (personality.friendliness < 0.3) {
        if (rand() % 3 == 0) response_text = "Look, " + response_text;
        else if (rand() % 3 == 1) response_text = "Ugh. " + response_text;
    }
    
    // Formality
    if (personality.formality > 0.8) {
        if (response_text.find("I don't") != std::string::npos) {
             response_text.replace(response_text.find("I don't"), 7, "I do not");
        }
        // Heuristic: Append "Sir" sometimes
        if (rand() % 5 == 0) response_text += ", Sir.";
    }

    // Continuous Learning: Save interesting interactions
    if (input_text.length() > 20 && response_text.length() > 10 && response_text.find("...") == std::string::npos) {
         // Only save if it's a "good" interaction (heuristic)
         // Append to a learning file
         std::ofstream learn_file("state/learned_interactions.txt", std::ios::app);
         if (learn_file) {
             learn_file << input_text << "|" << response_text << "\n";
             safe_print("[Persistence]: Saved interaction to state/learned_interactions.txt");
         } else {
             safe_print("[Persistence]: Failed to open file for writing!");
         }
    } else {
        // Debug why not saved (comment out in production)
         // safe_print("[Persistence]: Skipped. InLen=" + std::to_string(input_text.length()) + 
         //           " ResLen=" + std::to_string(response_text.length()) + " Res=" + response_text);
    }
    
    // Save Brain's response to context
    conversation_context.push_back("Brain: " + response_text);
    while (conversation_context.size() > MAX_CONTEXT_TURNS) conversation_context.pop_front();

    return response_text;
}

std::string Brain::get_associative_memory(const std::string& input) {
    if (!memory_store) return "";
    
    // Redis Cache Check
    if (redis_cache) {
        auto cached = redis_cache->get("assoc:" + input);
        if (cached) {
            emit_log("[Memory]: Cache HIT for context match.");
            return *cached;
        }
    }

    // 1. Try Entity Extraction first (High Precision)
    auto entities = extract_entities(input);
    for (const auto& entity : entities) {
        auto results = memory_store->query(entity);
        if (!results.empty()) {
             const auto& mem = results[0];
             std::string snippet = mem.content.substr(0, 300);
             if (mem.content.length() > 300) snippet += "...";
             std::string result = "I recall knowledge about " + entity + ". " + snippet;
             
             if (redis_cache) redis_cache->set("assoc:" + input, result, 300); // Cache for 5 mins
             return result;
        }
    }

    // 2. Fallback to basic keywords
    auto tokens = tokenize(input);
    for (const auto& word : tokens) {
        if (word.length() <= 3) continue; // Skip "the", "is", etc.
        
        auto results = memory_store->query(word);
        if (!results.empty()) {
            const auto& mem = results[0];
            std::string snippet = mem.content.substr(0, 300);
            if (mem.content.length() > 300) snippet += "...";
            std::string result = "I recall learning about " + word + ". " + snippet;

            if (redis_cache) redis_cache->set("assoc:" + input, result, 300);
            return result;
        }
    }
    // 3. Semantic Similarity (Word2Vec Phase 1)
    for (const auto& word : tokens) {
        if (word_embeddings.count(word)) {
            // Check Semantic Cache (e.g., sim:robot -> ai)
            std::string sim_cache_key = "sim:" + word;
            if (redis_cache) {
                auto cached_match = redis_cache->get(sim_cache_key);
                if (cached_match) {
                    auto results = memory_store->query(*cached_match);
                    if (!results.empty()) {
                        std::string result = "Reassociating " + word + " via " + (*cached_match) + ": " + results[0].content;
                        redis_cache->set("assoc:" + input, result, 300);
                        return result;
                    }
                }
            }

            // Find most similar base word
            std::string best_match = "";
            double max_sim = -1.0;
            
            for (const auto& [base, vec] : word_embeddings) {
                if (base == word) continue;
                double sim = 0;
                auto& w_vec = word_embeddings.at(word);
                for (size_t i = 0; i < VECTOR_DIM; ++i) sim += w_vec[i] * vec[i];
                
                if (sim > max_sim) {
                    max_sim = sim;
                    best_match = base;
                }
            }
            
            if (max_sim > 0.8) { // Similarity threshold
                if (redis_cache) redis_cache->set(sim_cache_key, best_match, 3600); // 1 hour cache
                
                auto results = memory_store->query(best_match);
                if (!results.empty()) {
                    emit_log("[Memory]: Semantic HIT - " + word + " relates to " + best_match);
                    std::string result = "Connecting " + word + " to my knowledge of " + best_match + ": " + results[0].content;
                    if (redis_cache) redis_cache->set("assoc:" + input, result, 300);
                    return result;
                }
            }
        }
    }

    return "";
}

double Brain::analyze_sentiment(const std::string& text) {
    auto tokens = tokenize(text);
    double score = 0.0;
    for (auto& t : tokens) {
        // Simple presence-based scoring
        if (std::find(positive_words.begin(), positive_words.end(), t) != positive_words.end()) score += 1.0;
        else if (std::find(negative_words.begin(), negative_words.end(), t) != negative_words.end()) score -= 1.0;
    }
    return score;
}

std::vector<std::string> Brain::extract_entities(const std::string& text) {
    std::vector<std::string> entities;
    
    // 1. Regex Extraction for specific patterns
    
    // Email
    std::regex email_regex(R"([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})");
    auto email_begin = std::sregex_iterator(text.begin(), text.end(), email_regex);
    auto email_end = std::sregex_iterator();
    for (std::sregex_iterator i = email_begin; i != email_end; ++i) {
        entities.push_back(i->str());
    }

    // Dates (ISO: YYYY-MM-DD or US: MM/DD/YYYY)
    std::regex date_regex(R"(\b\d{4}-\d{2}-\d{2}\b|\b\d{1,2}/\d{1,2}/\d{2,4}\b)");
    auto date_begin = std::sregex_iterator(text.begin(), text.end(), date_regex);
    for (std::sregex_iterator i = date_begin; i != email_end; ++i) {
         entities.push_back(i->str());
    }
    
    // Proper Noun Extraction (Fallback to Capitalization)
    std::stringstream ss(text);
    std::string word;
    std::string current_candidate;
    bool candidate_started_at_sentence_start = false;
    bool is_sentence_start = true;
    int words_in_candidate = 0;

    auto finalize_candidate = [&](std::string& candidate, bool started_at_start, int word_count) {
        if (candidate.empty()) return;
        
        bool keep = true;
        if (word_count == 1) {
            // Single word filtering
            if (started_at_start && is_stop_word(candidate)) keep = false;
            else if (candidate.length() <= 1) keep = false;
            // Filter "I" or "I'm" if stuck
            if (candidate == "I" || candidate == "I'm") keep = false; 
        }
        
        if (keep) {
            entities.push_back(candidate);
        }
        candidate.clear();
    };

    while (ss >> word) {
        bool ends_with_punct = !word.empty() && (word.back() == '.' || word.back() == '!' || word.back() == '?');
        
        std::string clean_word = word;
        while (!clean_word.empty() && !std::isalnum(clean_word.back())) clean_word.pop_back();
        while (!clean_word.empty() && !std::isalnum(clean_word.front())) clean_word.erase(0, 1);
        
        if (clean_word.empty()) {
            if (ends_with_punct) is_sentence_start = true;
            continue;
        }

        bool is_capitalized = std::isupper(clean_word[0]);
        bool is_all_caps = true;
        for (char c : clean_word) if (std::isalnum(c) && !std::isupper(c)) { is_all_caps = false; break; }

        if (is_capitalized || is_all_caps) {
            if (current_candidate.empty()) {
                candidate_started_at_sentence_start = is_sentence_start;
                words_in_candidate = 0;
            }
            if (!current_candidate.empty()) current_candidate += " ";
            current_candidate += clean_word;
            words_in_candidate++;
        } else {
            finalize_candidate(current_candidate, candidate_started_at_sentence_start, words_in_candidate);
        }
        
        is_sentence_start = ends_with_punct;
    }
    
    finalize_candidate(current_candidate, candidate_started_at_sentence_start, words_in_candidate);
    
    // Final deduplication
    std::sort(entities.begin(), entities.end());
    entities.erase(std::unique(entities.begin(), entities.end()), entities.end());
    
    return entities;
}

void Brain::load_stopwords() {
    std::ifstream file("data/stopwords.txt");
    if (!file.is_open()) {
        // Fallback or try parent dir
        file.open("../data/stopwords.txt");
    }
    
    if (file.is_open()) {
        std::string line;
        while (std::getline(file, line)) {
            // Trim
            line.erase(0, line.find_first_not_of(" \t\r\n"));
            line.erase(line.find_last_not_of(" \t\r\n") + 1);
            if (!line.empty()) {
                std::transform(line.begin(), line.end(), line.begin(), ::tolower);
                stopwords_.insert(line);
            }
        }
        safe_print("[Brain]: Loaded " + std::to_string(stopwords_.size()) + " stop words.");
    }
}

bool Brain::is_stop_word(const std::string& word) {
    if (stopwords_.empty()) return false;
    std::string lower = word;
    std::transform(lower.begin(), lower.end(), lower.begin(), ::tolower);
    return stopwords_.count(lower) > 0;
}



std::string Brain::decode_output(const std::vector<double>& logits) {
    std::string result = "";
    std::vector<std::pair<double, int>> scores;
    
    for(size_t i = 0; i < logits.size(); ++i) {
        scores.push_back({logits[i], static_cast<int>(i)});
    }
    
    // Sort descending
    std::sort(scores.begin(), scores.end(), [](auto& a, auto& b){ return a.first > b.first; });

    // Output top 3 words
    for(size_t i=0; i<3; ++i) {
        if (scores[i].first > 0.01) { // Threshold
           size_t idx = static_cast<size_t>(scores[i].second);
           if (vocab_decode.count(idx)) {
               result += vocab_decode[idx] + " ";
           }
        }
    }
    
    if (result.empty()) {
        std::vector<std::string> fallbacks = {
            "Processing internal neural mappings...",
            "Searching associative pathways...",
            "Analyzing cognitive inputs...",
            "Consulting long-term memory structures...",
            "Deliberating on sensory data...",
            "Synthesizing new synaptic connections..."
        };
        return fallbacks[static_cast<size_t>(rand()) % fallbacks.size()];
    }
    return result;
}

bool Brain::CheckPrintable(char c) {
    return (c >= 32 && c <= 126);
}

void Brain::sleep() {
    std::lock_guard<std::recursive_mutex> lock(brain_mutex);
    safe_print("[Brain is consolidating memories... zzz...]");
    
    // MEGA-BATCH 8: Enhanced Consolidation
    consolidate_memories();
    
    // Original Network Consolidation
    memory_center->network.consolidate_memories(memory_center->current_activity);
    cognitive_center->network.consolidate_memories(cognitive_center->current_activity);
    
    // Auto-save state
    save("state/brain_autosave.bin");
    
    // Restore stats
    emotions.energy = 1.0;
    emotions.happiness = std::min(1.0, emotions.happiness + 0.2);
    emotions.boredom = 0.0;
}

void Brain::update_reflex_learning(const std::string& trigger, double reward) {
    // Find key in reflex map that matches trigger (fuzzy or exact)
    // For simplicity, we assume we tracked the key, but we tracked the full input.
    // We'll rely on Reflex::reinforce checking containment/fuzzy again inside, 
    // or we should have stored the key. For now, try to reinforce with the input as "keyword"
    // CAUTION: Reflex::reinforce takes a keyword. We need to pass the keyword that triggered it.
    // Implementation Gaps: Reflex::get_reaction doesn't return the key.
    // Fix: We'll brute force reinforce for now or rely on an improved Reflex class later.
    // Actually, Reflex::reinforce iterates.
    // Let's iterate keys and find one that matches input.
    auto& instincts = reflex.get_instincts();
    std::string best_key;
    for(const auto& [key, val] : instincts) {
        if (trigger.find(key) != std::string::npos) {
            reflex.reinforce(key, last_reflex_response, reward); // Heuristic
            return;
        }
    }
}

void Brain::consolidate_memories() {
    if (!memory_store) return;
    
    // Move high-importance short-term memories to long-term SQL
    for (auto& item : conversation_history) {
        if (item.consolidated) continue;

        if (item.role == "User") {
            // Calculate Importance (Sentiment magnitude)
            double sentiment = std::abs(analyze_sentiment(item.text));
            if (sentiment > 0.5 || item.text.length() > 20) {
                 // Generate Embedding
                 std::vector<double> embedding(VECTOR_DIM, 0.0);
                 auto tokens = tokenize(item.text);
                 int count = 0;
                 for(const auto& t : tokens) {
                     if (word_embeddings.count(t)) {
                         dnn::add_vectors(embedding, word_embeddings[t]);
                         count++;
                     }
                 }
                 if (count > 0) {
                     for(auto& v : embedding) v /= count;
                 }
                 
                 // Store with embedding
                 // Using item.intent as key/category? Or just "Consolidated_Timestamp"
                 std::string key = "mem_" + std::to_string(item.timestamp) + "_" + std::to_string(rand() % 1000);
                 memory_store->store("Consolidated", item.text, "Sleep"); // Standard KV
                 memory_store->store_embedding(key, embedding); // Vector
                 
                 emit_log("[Memory]: Consolidated '" + item.text.substr(0, std::min((size_t)20, item.text.length())) + "...'");
            }
        }
        item.consolidated = true;
    }

    // Feature 3: Episodic Narrative Synthesis (Journaling)
    std::string summary;
    for (const auto& item : conversation_history) {
        summary += item.role + ": " + item.text + ". ";
    }
    if (!summary.empty()) {
        std::string key = "journal_" + std::to_string(std::time(nullptr));
        // In a real system, we'd use an LLM to summarize 'summary' here
        // For now, we store the raw log as a "Journal Entry"
        memory_store->store("Journal", summary, "Narrative");
        emit_log("[Memory]: Created Episodic Journal Entry.");
    }
    
    // Feature 6: REM Logic Trigger
    perform_rem_cycle();
}

std::string Brain::research(const std::string& topic) {
    // UNLOCK to allow main thread to be responsive
    {
        std::lock_guard<std::recursive_mutex> lock(brain_mutex);
        current_research_topic = topic;
        log_activity("[Background]: Researching " + topic + "...");
        if (on_research_update) on_research_update("Starting research on: " + topic);
    }
    
    // Fetch with links (network bound)
    auto result = research_tools::fetch_comprehensive(topic);
    
    // RE-LOCK to update state
    std::lock_guard<std::recursive_mutex> lock(brain_mutex);
    std::string content = result.summary;
    
    // Add sub-topics to queue (if interesting/unique)
    int added = 0;
    for (const auto& sub : result.related_topics) {
        if (added < 5 && std::find(learned_topics.begin(), learned_topics.end(), sub) == learned_topics.end()) {
            if (sub.find("List of") == std::string::npos && sub.find("Wikipedia") == std::string::npos) {
                 research_queue.push_back(sub);
                 added++;
            }
        }
    }
    learned_topics.push_back(topic);
    
    if (content.find("No information found") != std::string::npos || content.find("Connection Failed") != std::string::npos) {
        return content;
    }

    log_activity("[Background]: Read " + std::to_string(content.length()) + " chars on " + topic);
    
    // Feed the content into the brain to "learn" it
    std::stringstream ss(content);
    std::string segment;
    int count = 0;
    while(std::getline(ss, segment, '.')) {
        if (segment.length() < 3) continue;
        
        // "Read" (interact calls interact which calls reinforce)
        interact(segment); // This reinforces new words!
        // "Reinforce" highly
        memory_center->network.consolidate_memories(memory_center->current_activity);
        
        if (++count > 5) break; 
    }
    
    // Save to Explicit Memory
    if (memory_store) {
        memory_store->store("Research", content, topic);
    }
    
    if (on_research_update) on_research_update("Completed research on: " + topic);
    return "I learned about " + topic + "! " + content.substr(0, 50) + "... (Found " + std::to_string(result.related_topics.size()) + " related topics)";
}

void Brain::teach(const std::string& input_text, const std::string& target_text) {
    std::lock_guard<std::recursive_mutex> lock(brain_mutex);
    
    // 1. Sensual Perception (Encoding) - Same as interact
    std::vector<double> input_vec(VOCAB_SIZE, 0.0);
    auto input_tokens = tokenize(input_text);
    for (const auto& word : input_tokens) {
        std::hash<std::string> hasher;
        size_t idx = hasher(word) % VOCAB_SIZE;
        input_vec[idx] += 1.0;
        if (vocab_decode.find(idx) == vocab_decode.end()) vocab_decode[idx] = word;
    }
    // Normalize Input
    double max_val = 1.0; 
    for(double v : input_vec) if(v > max_val) max_val = v;
    for(auto& v : input_vec) v /= max_val;

    // 2. Prepare Target Vector
    std::vector<double> target_vec(VOCAB_SIZE, 0.0);
    auto target_tokens = tokenize(target_text);
    for (const auto& word : target_tokens) {
        std::hash<std::string> hasher;
        size_t idx = hasher(word) % VOCAB_SIZE;
        target_vec[idx] += 1.0;
        if (vocab_decode.find(idx) == vocab_decode.end()) vocab_decode[idx] = word;
    }
    // Normalize Target
    double t_max = 1.0;
    for(double v : target_vec) if(v > t_max) t_max = v;
    for(auto& v : target_vec) v /= t_max;

    // 3. Forward Pass to generate "Thought"
    std::vector<double> thought = language_encoder->process(input_vec);
    std::vector<double> memory_context = memory_center->process(thought);
    
    std::vector<double> cognitive_input = thought;
    cognitive_input.insert(cognitive_input.end(), memory_context.begin(), memory_context.end());
    
    std::vector<double> response_thought = cognitive_center->process(cognitive_input);

    // 4. Supervised Training of Decoder
    // We want decoder(response_thought) -> target_vec
    // Train it multiple times to sink it in
    for(int i=0; i<5; ++i) {
        language_decoder->train(response_thought, target_vec, 0.1); 
    }
    
    // 5. Reinforce the path that got us here
    language_encoder->reinforce(0.05);
    memory_center->reinforce(0.05);
    cognitive_center->reinforce(0.05);
    
    // safe_print("Learned: " + input_text + " -> " + target_text);
}



std::string Brain::deep_research(const std::string& topic) {
    std::lock_guard<std::recursive_mutex> lock(brain_mutex);
    // Clear queue to focus on this
    research_queue.clear();
    
    // Initial fetches
    return research(topic);
}

long long Brain::get_knowledge_size() {
    if (memory_store) {
        return memory_store->get_memory_count() * 1024; // Approximation for now
    }
    return 0;
}

void Brain::automata_loop() {
    while (running) {
        std::this_thread::sleep_for(std::chrono::seconds(2)); // Tick every 2s

        // 1. Evaluate Needs & Generate Goals
        evaluate_goals();
        
        // 1b. Update Sensory Focus (Task #39)
        update_sensory_focus();

        // 2. Execute Tasks
        Task* current = task_manager.get_next_task();
        if (current) {
            std::string desc = current->description;
            emit_log("[Cognition]: Executing #" + std::to_string(current->id) + ": " + desc);
            
            if (current->type == TaskType::RESEARCH) {
                std::string topic = desc.length() > 9 ? desc.substr(9) : "unknown";
                research(topic);
            } 
            else if (current->type == TaskType::SLEEP) {
                sleep();
                std::lock_guard<std::recursive_mutex> lock(brain_mutex);
                emotions.energy = 1.0;
                hormones.melatonin = 0.0;
            }
            else if (current->type == TaskType::EAT) {
                std::this_thread::sleep_for(std::chrono::seconds(1));
                std::lock_guard<std::recursive_mutex> lock(brain_mutex);
                metabolism.hunger = 0.0;
                metabolism.glucose = 1.0;
                hormones.dopamine = std::min(1.0, hormones.dopamine + 0.3);
            }
            else if (current->type == TaskType::DRINK) {
                std::this_thread::sleep_for(std::chrono::seconds(1));
                std::lock_guard<std::recursive_mutex> lock(brain_mutex);
                metabolism.thirst = 0.0;
                hormones.serotonin = std::min(1.0, hormones.serotonin + 0.2);
            }
            else if (current->type == TaskType::MOTORS) {
                if (ros_bridge) {
                    dnn::infra::JointState js;
                    js.name = {"arm_joint_1", "arm_joint_2"};
                    js.position = {0.5, -0.2};
                    ros_bridge->publish_joint_command(js);
                }
            }
            
            task_manager.complete_active_task();
        }

        // 3. Biological Pulse
        metabolize_step();
        
        // 4. State Regulation
        {
             std::lock_guard<std::recursive_mutex> lock(brain_mutex);
             
             // Serotonin stabilizes emotions
             double stab = 0.01 + (hormones.serotonin * 0.02);
             if (emotions.happiness > 0.5) emotions.happiness -= stab;
             else if (emotions.happiness < 0.5) emotions.happiness += stab;
             
             // Cortisol increases anger/fear
             if (hormones.cortisol > 0.5) {
                 emotions.anger = std::min(1.0, emotions.anger + 0.05);
                 emotions.fear = std::min(1.0, emotions.fear + 0.03);
             }

             if (on_emotion_update) {
                std::string status = "Env: " + std::to_string(int(environment.time_of_day)) + "h | ";
                status += "Energy: " + std::to_string(int(emotions.energy*100)) + "% | ";
                status += "Hunger: " + std::to_string(int(metabolism.hunger*100)) + "% | ";
                status += "Dopamine: " + std::to_string(int(hormones.dopamine*100)) + "%";
                on_emotion_update(status);
             }
        }
    }
}

std::string Brain::get_status() {
    std::stringstream ss;
    ss << "--- Brain Status ---\n";
    ss << "Energy: " << (emotions.energy * 100) << "%\n";
    ss << "Happiness: " << (emotions.happiness * 100) << "%\n";
    ss << "Boredom: " << (emotions.boredom * 100) << "%\n";
    ss << "Current Thought: " << current_thought << "\n";
    ss << "--------------------";
    return ss.str();
}

std::string Brain::get_json_state() {
    std::lock_guard<std::recursive_mutex> lock(brain_mutex);
    std::stringstream ss;
    ss << "{";
    ss << "\"personality\": {";
    ss << "\"curiosity\": " << personality.curiosity << ",";
    ss << "\"playfulness\": " << personality.playfulness << ",";
    ss << "\"friendliness\": " << personality.friendliness << ",";
    ss << "\"formality\": " << personality.formality << ",";
    ss << "\"positivity\": " << personality.positivity;
    ss << "},";
    ss << "\"emotions\": {";
    ss << "\"happiness\": " << emotions.happiness << ",";
    ss << "\"sadness\": " << emotions.sadness << ",";
    ss << "\"anger\": " << emotions.anger << ",";
    ss << "\"fear\": " << emotions.fear << ",";
    ss << "\"energy\": " << emotions.energy << ",";
    ss << "\"boredom\": " << emotions.boredom;
    ss << "},";
    ss << "\"sensory_activity\": [";
    for (size_t i = 0; i < sensory_inputs.size(); ++i) {
        ss << "{";
        ss << "\"name\": \"" << sensory_inputs[i]->name() << "\",";
        ss << "\"type\": " << (int)sensory_inputs[i]->type() << ",";
        ss << "\"focus\": " << sensory_inputs[i]->get_focus() << ",";
        ss << "\"activity\": [";
        auto act = sensory_inputs[i]->get_current_activity();
        for (size_t j = 0; j < act.size(); ++j) {
            ss << act[j] << (j < act.size() - 1 ? "," : "");
        }
        ss << "]";
        ss << "}" << (i < sensory_inputs.size() - 1 ? "," : "");
    }
    ss << "],";
    ss << "\"thought\": \"" << current_thought << "\",";
    ss << "\"learning\": {";
    ss << "\"focus_topic\": \"" << focus_topic << "\",";
    ss << "\"focus_level\": " << focus_level << ",";
    ss << "\"learned_count\": " << learned_topics.size();
    ss << "},";
    ss << "\"metadata\": {";
    ss << "\"knowledge_size\": " << get_knowledge_size() << ",";
    ss << "\"uptime\": " << (std::chrono::duration_cast<std::chrono::seconds>(std::chrono::steady_clock::now().time_since_epoch()).count()) << ",";
    ss << "\"version\": \"2.1.0-alpha\"";
    ss << "}";
    ss << "}";
    return ss.str();
}

void Brain::perform_rem_cycle() {
    safe_print("[Brain]: Entering REM Cycle (Dreaming)...");
    
    // Feature 6: Replay memories with high emotional weight
    // For now, randomly selecting words from 'learned_topics' as triggers
    if (learned_topics.empty()) return;
    
    std::random_device rd;
    std::mt19937 gen(rd());
    std::uniform_int_distribution<> dis(0, learned_topics.size() - 1);
    
    for (int i = 0; i < 5; ++i) { // 5 Dream Sequences
        // Simulate random neural activation
        std::string dream_trigger = learned_topics[dis(gen)];
        std::string dream = get_associative_memory(dream_trigger);
        
        // Reinforce connections (Hebbian Learning Stub)
        // In a full implementation, we would train the neural net with this input
        if (memory_center) {
            // Need vector conversion helpers, assuming generic train for now
            // memory_center->train(...) 
        }
        safe_print("[Dreaming]: " + dream_trigger + " -> " + dream.substr(0, 30) + "...");
    }
}

void Brain::metabolize_step() {
    std::lock_guard<std::recursive_mutex> lock(brain_mutex);
    
    // 1. Base metabolism and energy consumption
    double basic_rate = 0.0005;
    if (environment.is_daylight) basic_rate *= 1.2; 
    
    if (task_manager.has_pending_tasks()) basic_rate *= 2.0;
    
    emotions.energy -= basic_rate;
    metabolism.glucose -= basic_rate * 0.5;
    
    // 2. Hunger and Thirst dynamics
    metabolism.hunger = std::min(1.0, metabolism.hunger + 0.002);
    metabolism.thirst = std::min(1.0, metabolism.thirst + 0.003);
    
    // High hunger/thirst increases Cortisol (stress)
    if (metabolism.hunger > 0.7 || metabolism.thirst > 0.7 || emotions.energy < 0.2) {
        hormones.cortisol = std::min(1.0, hormones.cortisol + 0.02);
    } else {
        hormones.cortisol = std::max(0.0, hormones.cortisol - 0.01);
    }
    
    // 3. Glucose affects energy
    if (metabolism.glucose < 0.3) {
        emotions.energy = std::max(0.0, emotions.energy - 0.005);
    }

    // 4. Day/Night Cycle Update
    environment.time_of_day += (1.0 / 30.0); 
    if (environment.time_of_day >= 24.0) environment.time_of_day = 0.0;
    
    environment.is_daylight = (environment.time_of_day >= 6.0 && environment.time_of_day <= 18.0);
    
    // Melatonin production (sleep trigger)
    if (!environment.is_daylight) {
        hormones.melatonin = std::min(1.0, hormones.melatonin + 0.03);
    } else {
        hormones.melatonin = std::max(0.0, hormones.melatonin - 0.05);
    }
    
    // Melatonin causes sleepiness
    if (hormones.melatonin > 0.5) {
        emotions.energy = std::max(0.0, emotions.energy - 0.01);
    }

    // 5. Metacognition influence
    if (metacognition) {
        metacognition->monitor_performance(emotions.happiness, emotions.boredom);
    }
}

void Brain::update_from_json(const std::string& json) {
    std::lock_guard<std::recursive_mutex> lock(brain_mutex);
    // Extremely basic parser for "key": value
    auto parse_val = [&](const std::string& key) -> double {
        size_t pos = json.find("\"" + key + "\"");
        if (pos == std::string::npos) return -1.0;
        size_t colon = json.find(":", pos);
        if (colon == std::string::npos) return -1.0;
        return std::stod(json.substr(colon + 1));
    };
    
    // Personality
    double val;
    if ((val = parse_val("curiosity")) >= 0) personality.curiosity = val;
    if ((val = parse_val("playfulness")) >= 0) personality.playfulness = val;
    if ((val = parse_val("friendliness")) >= 0) personality.friendliness = val;
    if ((val = parse_val("formality")) >= 0) personality.formality = val;
    if ((val = parse_val("positivity")) >= 0) personality.positivity = val;
    
    // Emotions
    if ((val = parse_val("happiness")) >= 0) emotions.happiness = val;
    if ((val = parse_val("sadness")) >= 0) emotions.sadness = val;
    if ((val = parse_val("anger")) >= 0) emotions.anger = val;
    if ((val = parse_val("fear")) >= 0) emotions.fear = val;
    if ((val = parse_val("energy")) >= 0) emotions.energy = val;
    if ((val = parse_val("boredom")) >= 0) emotions.boredom = val;
    
    emit_log("[Brain]: State Updated via API");
}


void Brain::save(const std::string& filename) {
    std::lock_guard<std::recursive_mutex> lock(brain_mutex);
    safe_print("[Brain]: Saving memory state to " + filename + "...");

    std::ofstream os(filename, std::ios::binary);
    if (!os) return;

    os.write(reinterpret_cast<char*>(&personality), sizeof(Personality));
    os.write(reinterpret_cast<char*>(&emotions), sizeof(Emotions));
    
    // Save Vocab
    size_t vocab_count = vocab_decode.size();
    os.write(reinterpret_cast<char*>(&vocab_count), sizeof(size_t));
    for (const auto& [idx, word] : vocab_decode) {
        os.write(reinterpret_cast<const char*>(&idx), sizeof(size_t));
        size_t len = word.length();
        os.write(reinterpret_cast<const char*>(&len), sizeof(size_t));
        os.write(word.c_str(), len);
    }
    
    language_encoder->network.save(os);
    language_decoder->network.save(os);
    memory_center->network.save(os);
    cognitive_center->network.save(os);
    
    // Save Reflex weights
    auto& instincts = reflex.get_instincts();
    size_t reflex_count = instincts.size();
    os.write(reinterpret_cast<char*>(&reflex_count), sizeof(size_t));
    for (const auto& [key, choices] : instincts) {
        size_t klen = key.length();
        os.write(reinterpret_cast<char*>(&klen), sizeof(size_t));
        os.write(key.c_str(), klen);
        
        size_t clen = choices.size();
        os.write(reinterpret_cast<char*>(&clen), sizeof(size_t));
        for (const auto& choice : choices) {
            size_t tlen = choice.text.length();
            os.write(reinterpret_cast<char*>(&tlen), sizeof(size_t));
            os.write(choice.text.c_str(), tlen);
            os.write(reinterpret_cast<const char*>(&choice.weight), sizeof(double));
        }
    }
    
    safe_print("[Brain]: Saved.");
}

void Brain::load(const std::string& filename) {
    std::lock_guard<std::recursive_mutex> lock(brain_mutex);
    std::ifstream is(filename, std::ios::binary);
    if (!is) {
        safe_print("[Brain]: Could not load file: " + filename);
        return;
    }

    safe_print("[Brain]: Loading memory state from " + filename + "...");

    is.read(reinterpret_cast<char*>(&personality), sizeof(Personality));
    is.read(reinterpret_cast<char*>(&emotions), sizeof(Emotions));
    
    // Load Vocab
    size_t vocab_count = 0;
    if (is.read(reinterpret_cast<char*>(&vocab_count), sizeof(size_t))) {
        for (size_t i = 0; i < vocab_count; ++i) {
            size_t idx, len;
            is.read(reinterpret_cast<char*>(&idx), sizeof(size_t));
            is.read(reinterpret_cast<char*>(&len), sizeof(size_t));
            std::string word(len, ' ');
            is.read(&word[0], len);
            vocab_decode[idx] = word;
        }
    }
    
    language_encoder->network.load(is);
    language_decoder->network.load(is);
    memory_center->network.load(is);
    cognitive_center->network.load(is);
    
    // Load Reflex weights
    size_t reflex_count = 0;
    if (is.read(reinterpret_cast<char*>(&reflex_count), sizeof(size_t))) {
        auto& instincts = reflex.get_instincts();
        for (size_t i = 0; i < reflex_count; ++i) {
            size_t klen;
            is.read(reinterpret_cast<char*>(&klen), sizeof(size_t));
            std::string key(klen, ' ');
            is.read(&key[0], klen);
            
            size_t clen;
            is.read(reinterpret_cast<char*>(&clen), sizeof(size_t));
            std::vector<Reflex::WeightedResponse> choices;
            for (size_t j = 0; j < clen; ++j) {
                size_t tlen;
                is.read(reinterpret_cast<char*>(&tlen), sizeof(size_t));
                std::string text(tlen, ' ');
                is.read(&text[0], tlen);
                double weight;
                is.read(reinterpret_cast<char*>(&weight), sizeof(double));
                choices.push_back({text, weight});
            }
            instincts[key] = choices;
        }
    }
    
    safe_print("[Brain]: Memories restored.");
}

// Helpers
void Brain::log_activity(const std::string& msg) {
    Logger::instance().log(msg);
    emit_log(msg); // Send to Server/Dashboard!
}

std::vector<std::string> Brain::tokenize(const std::string& text) {
    std::vector<std::string> tokens;
    std::string current;
    for (char c : text) {
        if (std::isalnum(c)) {
            current += static_cast<char>(std::tolower(c));
        } else if (!current.empty()) {
            tokens.push_back(current);
            current.clear();
        }
    }
    if (!current.empty()) tokens.push_back(current);
    return tokens;
}



// Feature 9: Entropy-Based Curiosity
std::string Brain::find_curiosity_topic() {
    std::lock_guard<std::recursive_mutex> lock(brain_mutex);
    
    // Scan 'learned_topics' and find one with sparse connections in memory_store
    // Simulating "Entropy" as inverse of connection count
    
    std::string best_topic;
    double max_entropy = -1.0;
    
    for (const auto& topic : learned_topics) {
        // Query memory store for related items
        // Low results = High Entropy (Unknown)
        auto results = memory_store->query(topic);
        double entropy = 1.0 / (1.0 + results.size());
        
        if (entropy > max_entropy) {
            max_entropy = entropy;
            best_topic = topic;
        }
    }
    
    if (best_topic.empty() && !learned_topics.empty()) {
        best_topic = learned_topics[rand() % learned_topics.size()];
    }
    if (best_topic.empty()) best_topic = "quantum_physics"; // Default
    
    return best_topic;
}

std::string Brain::get_memory_graph() {
    std::lock_guard<std::recursive_mutex> lock(brain_mutex);
    if (!memory_store) return "{\"nodes\":[], \"links\":[]}";
    return memory_store->get_graph_json(50);
}

void Brain::evaluate_goals() {
    std::lock_guard<std::recursive_mutex> lock(brain_mutex);
    
    if (task_manager.has_pending_tasks()) return;

    // Emotional/Biological State influence
    struct GoalCandidate { 
        std::string name; 
        double score; 
        std::string param;
    };
    std::vector<GoalCandidate> goals;

    // 1. Biological Needs Scoring
    double sleep_score = (1.0 - emotions.energy) * 4.0 + (hormones.melatonin * 2.0); 
    goals.push_back({"SLEEP", sleep_score, ""});
    
    double eat_score = metabolism.hunger * 5.0 + (hormones.cortisol * 1.5);
    goals.push_back({"EAT", eat_score, ""});
    
    double drink_score = metabolism.thirst * 6.0 + (hormones.cortisol * 1.5);
    goals.push_back({"DRINK", drink_score, ""});

    // 2. Cognitive Needs Scoring
    double research_score = (emotions.boredom * 3.0) + (personality.curiosity * 1.5) + (hormones.dopamine * 2.0);
    if (emotions.energy < 0.2) research_score *= 0.1; 
    goals.push_back({"RESEARCH", research_score, ""});
    
    double social_score = (emotions.energy * 0.5) + (personality.friendliness * 0.5) + (hormones.serotonin * 1.0);
    goals.push_back({"INTERACTION", social_score, "ASK_QUESTION"});

    // 3. MCTS Advisor
    std::string mcts_choice = planning_unit->decide_best_action(focus_topic, emotions.energy, emotions.boredom, metabolism.hunger, metabolism.thirst);
    for(auto& g : goals) {
        if(g.name == mcts_choice) g.score += 2.0;
    }

    // 4. Selection
    std::sort(goals.begin(), goals.end(), [](const GoalCandidate& a, const GoalCandidate& b){ return a.score > b.score; });
    GoalCandidate winner = goals[0];
    
    if (winner.score < 0.5) return;
    
    // 5. Execution Dispatch
    if (winner.name == "RESEARCH") {
         std::string topic = find_curiosity_topic();
         task_manager.add_task("Research " + topic, TaskType::RESEARCH, TaskPriority::LOW);
         emit_thought("Goal: Researching " + topic + " (Score: " + std::to_string(winner.score) + ")");
    } else if (winner.name == "SLEEP") {
         task_manager.add_task("Sleep Cycle", TaskType::SLEEP, TaskPriority::MEDIUM);
         emit_thought("Goal: Sleeping (Score: " + std::to_string(winner.score) + ")");
    } else if (winner.name == "EAT") {
         task_manager.add_task("Foraging/Feeding", TaskType::EAT, TaskPriority::HIGH);
         emit_thought("Goal: Eating (Cortisol high, hunger high)");
    } else if (winner.name == "DRINK") {
         task_manager.add_task("Hydrating", TaskType::DRINK, TaskPriority::HIGH);
         emit_thought("Goal: Drinking (Thirst critical)");
    } else if (winner.name == "INTERACTION") {
         task_manager.add_task("Engagement: " + winner.param, TaskType::INTERACTION, TaskPriority::LOW);
         emit_thought("Goal: Interaction (Score: " + std::to_string(winner.score) + ")");
    }
}

// Mega-Batch 7: Context & NLU features

void Brain::update_context(const std::string& role, const std::string& text, const std::string& intent) {
    if (conversation_history.size() >= 5) {
        conversation_history.pop_front();
    }
    conversation_history.push_back({role, text, intent, std::time(nullptr), false});

    // Feature 2: Theory of Mind Update
    if (role == "User") {
        double sentiment = analyze_sentiment(text);
        // Simple trust update: consistency builds trust
        user_model.trust = std::min(1.0, user_model.trust + 0.001); 
        user_model.estimated_happiness = (user_model.estimated_happiness * 0.8) + (sentiment * 0.2);
        user_model.intent_history.push_back(intent);
        if (user_model.intent_history.size() > 10) user_model.intent_history.pop_front();
    }

    // Feature 10: Classical Conditioning
    // If we hear "Bell" (trigger) and then receive "Reward" (positive sentiment),
    // associate Bell -> Reward.
    if (role == "User" && conversation_history.size() > 1) {
        const auto& prev = conversation_history[conversation_history.size()-2];
        double sentiment = analyze_sentiment(text);
        if (sentiment > 0.8 && prev.role == "User") {
            // Previous input led to reward? Or simple co-occurrence?
            // Simplified: If sentiment is high, reinforce previous non-sentiment tokens
            auto tokens = tokenize(prev.text);
            if (!tokens.empty()) {
                std::string potential_trigger = tokens[0]; // Naive
                condition_map[potential_trigger] = "POSITIVE_RESPONSE";
            }
        }
    }
}

std::string Brain::resolve_intent(const std::string& text) {
    std::string resolved = text;
    
    if (conversation_history.empty()) return resolved;

    // 1. Short Follow-up Context Injection
    // Matches "Why?", "And then?", "But why?"
    // If text is short and starts with connector/question
    bool is_short = text.length() < 20;
    std::regex follow_up_pattern(R"(^(Why|why|How about|how about|And|and|But|but|What about|what about).*)");
    if (is_short && std::regex_match(text, follow_up_pattern)) {
        const auto& last = conversation_history.back();
        resolved += " (Context: " + last.text + ")";
    }

    // 2. Pronoun Resolution (He/She/It/This/That)
    // We look for the most recent entity in conversation history.
    std::regex pronoun_regex(R"(\b(he|He|she|She|it|It|this|This|that|That|they|They)\b)");
    if (std::regex_search(text, pronoun_regex)) {
        // Search backwards for an entity
        std::string target_entity;
        for (auto it = conversation_history.rbegin(); it != conversation_history.rend(); ++it) {
            std::vector<std::string> entities = extract_entities(it->text);
            if (!entities.empty()) {
                // Pick the first entity as candidate
                // Improvements: Distinction between persons/objects based on pronoun mapping
                target_entity = entities[0];
                break;
            }
        }
        
        if (!target_entity.empty()) {
             // Instead of blind replace which might be wrong, we append context reference
             // Blind replace: "He went..." -> "Bob went..." is risky if multiple pronouns.
             // Safer: Append "[Refers to: Bob]"
             resolved += " [Refers to: " + target_entity + "]";
        }
    }
    
    return resolved;
}

std::vector<std::string> Brain::find_similar_concepts(const std::string& term) {
    if (!memory_store) return {};
    auto vec = memory_store->retrieve_embedding(term); // Exact match first
    if (vec.empty()) {
        // Fallback: try lowercase
        std::string lower = term;
        std::transform(lower.begin(), lower.end(), lower.begin(), ::tolower);
        vec = memory_store->retrieve_embedding(lower);
    }
    if (vec.empty()) return {}; 
    return memory_store->search_similar(vec, 5);
}

void Brain::register_sensory_unit(std::unique_ptr<dnn::SensoryUnit> unit) {
    std::lock_guard<std::recursive_mutex> lock(brain_mutex);
    sensory_inputs.push_back(std::move(unit));
    log_activity("[Sensory]: Registered unit: " + sensory_inputs.back()->name());
}

void Brain::update_sensory_focus() {
    std::lock_guard<std::recursive_mutex> lock(brain_mutex);
    // Task #39: Focus Mechanism
    // If we are in "Research" mode or "Deep Scan", increase vision/lidar focus
    // If we are in "Interaction" mode, increase audio focus
    
    std::string intent = "";
    if (!conversation_history.empty()) intent = conversation_history.back().intent;

    for (auto& unit : sensory_inputs) {
        double target_focus = 0.5; // Baseline
        
        if (unit->type() == dnn::SensoryType::Vision) {
            if (intent == "SCENE_ANALYSIS" || focus_topic != "None") target_focus = 0.9;
            emit_neural_event("sensory_focus", "Vision focus adjusted to " + std::to_string(target_focus));
        } else if (unit->type() == dnn::SensoryType::Audio) {
            if (intent == "LISTENING" || intent == "CHAT") target_focus = 0.9;
            emit_neural_event("sensory_focus", "Audio focus adjusted to " + std::to_string(target_focus));
        } else if (unit->type() == dnn::SensoryType::Internal) {
            // Clock/Internal usually have constant focus unless we are "Meditating"
            target_focus = 0.7;
        }

        // Smooth adjustment toward target
        double current = unit->get_focus();
        unit->set_focus(current * 0.8 + target_focus * 0.2);
    }
}

void Brain::emit_neural_event(const std::string& type, const std::string& data) {
    if (bypass_enabled && on_neural_event) {
        on_neural_event(type, data);
    }
}

std::vector<double> Brain::get_aggregate_sensory_input() {
    std::lock_guard<std::recursive_mutex> lock(brain_mutex);
    std::vector<double> aggregate(VECTOR_DIM, 0.0);
    
    double total_weight = 0.0;
    for (const auto& unit : sensory_inputs) {
        if (!unit->is_active()) continue;
        
        std::vector<double> act = unit->get_current_activity();
        if (act.size() != VECTOR_DIM) continue;
        
        double focus = unit->get_focus();
        for (size_t i = 0; i < VECTOR_DIM; ++i) {
            aggregate[i] += act[i] * focus;
        }
        total_weight += focus;
    }
    
    if (total_weight > 0.0) {
        for (double& val : aggregate) val /= total_weight;
    }
    
    return aggregate;
}

// ========== COGNITIVE CORE METHOD IMPLEMENTATIONS ==========

std::string Brain::deep_reason(const std::string& query, const std::vector<std::string>& context) {
    std::lock_guard<std::recursive_mutex> lock(brain_mutex);
    
    if (!cognitive_core) {
        return "Cognitive core not initialized";
    }
    
    emit_thought("Deep reasoning about: " + query);
    
    auto result = cognitive_core->reason(query, context);
    
    std::string response = result.conclusion;
    if (!result.explanation.empty()) {
        response += "\n\nExplanation: " + result.explanation;
    }
    response += "\n(Confidence: " + std::to_string(int(result.confidence * 100)) + "%)";
    
    return response;
}

float Brain::analyze_causality(const std::string& cause, const std::string& effect) {
    std::lock_guard<std::recursive_mutex> lock(brain_mutex);
    
    if (!cognitive_core) {
        return 0.0f;
    }
    
    emit_thought("Analyzing causal relationship: " + cause + "  " + effect);
    
    return cognitive_core->compute_causal_effect(cause, effect);
}

std::string Brain::what_if(const std::string& variable, float new_value, const std::string& target) {
    std::lock_guard<std::recursive_mutex> lock(brain_mutex);
    
    if (!cognitive_core) {
        return "Cognitive core not initialized";
    }
    
    emit_thought("Counterfactual reasoning: What if " + variable + " = " + std::to_string(new_value) + "?");
    
    return cognitive_core->counterfactual_reasoning(variable, new_value, target);
}

std::vector<std::string> Brain::query_commonsense(const std::string& subject, const std::string& relation) {
    std::lock_guard<std::recursive_mutex> lock(brain_mutex);
    
    if (!cognitive_core) {
        return {};
    }
    
    emit_thought("Querying commonsense knowledge about: " + subject);
    
    return cognitive_core->query_commonsense(subject, relation);
}

void Brain::adapt_from_examples(const std::vector<std::pair<std::vector<float>, std::vector<float>>>& examples) {
    std::lock_guard<std::recursive_mutex> lock(brain_mutex);
    
    if (!cognitive_core) {
        return;
    }
    
    emit_thought("Meta-learning from " + std::to_string(examples.size()) + " examples");
    
    cognitive_core->meta_learn(examples);
    
    safe_print("[Brain]: Adapted from " + std::to_string(examples.size()) + " examples via meta-learning");
}

std::string Brain::get_cognitive_status() {
    std::lock_guard<std::recursive_mutex> lock(brain_mutex);
    
    if (!cognitive_core) {
        return "Cognitive core: Not initialized";
    }
    
    auto status = cognitive_core->get_status();
    
    std::string report = "=== Cognitive Core Status ===\n";
    report += "Memories: " + std::to_string(status.total_memories) + "\n";
    report += "Knowledge Triples: " + std::to_string(status.knowledge_triples) + "\n";
    report += "Current Reasoning: " + status.current_reasoning + "\n";
    report += "Confidence: " + std::to_string(int(status.overall_confidence * 100)) + "%\n";
    report += "\nSystems Online:\n";
    report += "   Causal Reasoning\n";
    report += "   Counterfactual Inference\n";
    report += "   Abductive Reasoning\n";
    report += "   Explanation Generation\n";
    report += "   Common-Sense Knowledge\n";
    report += "   Meta-Learning\n";
    report += "   Visual Perception\n";
    report += "   Audio Understanding\n";
    report += "   Distributed Intelligence\n";
    
    return report;
}
